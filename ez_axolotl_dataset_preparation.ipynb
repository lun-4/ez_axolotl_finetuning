{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"mount_file_id":"12DWyYQFKOli1m1Ay_A2awBXnDp_OUSRY","authorship_tag":"ABX9TyPduqnGu71TUU+Xo14mSN9Y"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["this is the dataset preparation script, you run this first, configure the parameters of your dataset, and at the end you'll have something ready for the finetuning script\n","\n","---\n","\n","1. connect to a runtime\n","2. click on the folder on the left side, connect the runtime to your google drive\n","3. set `dataset_type` to your dataset's type\n","4. set `dataset_path` to your dataset's path (should start with `/content/drive/MyDrive` if you're using drive linking)\n","5. run the cell (click play)"],"metadata":{"id":"ULAFmyfQjfV3"}},{"cell_type":"code","execution_count":8,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"32xHAVW_jcXt","executionInfo":{"status":"ok","timestamp":1687227610168,"user_tz":180,"elapsed":400,"user":{"displayName":"Lunar Lander","userId":"16012928837852072656"}},"outputId":"9341c624-cd98-48e1-ba99-d00879a00aa6"},"outputs":[{"output_type":"stream","name":"stdout","text":["there are 42 files\n","processing with split by file\n","done! validating dataset can be read...\n","done for real now\n"]}],"source":["dataset_type = \"raw\" #@param [\"raw\"]\n","dataset_path = \"/content/drive/MyDrive/test-dataset\" #@param {\"type\": \"string\"}\n","\n","split_by = \"file\" #@param [\"word_count\", \"file\"]\n","\n","#@markdown this is only used on word_count split mode\n","split_by_word_count = 200 #@param {\"type\":\"integer\"}\n","\n","#@markdown This will be the output folder of the combined dataset, you'll point the finetune script to this folder\n","output_path_string = \"/content/drive/MyDrive/ez_axolotl_dataset\" #@param {\"type\":\"string\"}\n","\n","import json\n","from pathlib import Path\n","\n","output_path = Path(output_path_string)\n","output_path.mkdir(exist_ok=True)\n","\n","file_paths = []\n","for filepath in Path(dataset_path).glob('**/*'):\n","  file_paths.append(filepath)\n","\n","print('there are', len(file_paths), 'files')\n","\n","output_jsonl = output_path / 'dataset.jsonl'\n","\n","# TODO support word_count\n","assert split_by == 'file'\n","\n","print('processing with split by', split_by)\n","\n","with output_jsonl.open(mode='w') as output_fd:\n","  for path in file_paths:\n","    with path.open() as input_fd:\n","      data_line = {'text': input_fd.read()}\n","      output_fd.write(json.dumps(data_line)+'\\n')\n","      output_fd.flush()\n","\n","print('done! validating dataset can be read...')\n","\n","\n","with output_jsonl.open(mode='r') as output_fd:\n","  for line in output_fd:\n","    json.loads(line)\n","\n","print('done for real now')\n"]}]}